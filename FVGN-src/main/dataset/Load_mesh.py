"""Utility functions for reading the datasets."""

import sys
import os

current_file_path = os.path.split(os.path.split(__file__)[0])[0]
sys.path.append(current_file_path)

from utils import utilities, noise
from utils.utilities import NodeType

import matplotlib

matplotlib.use("Agg")
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

import numpy as np
import torch
from torch.utils.data import IterableDataset
from torch_geometric.data import Dataset
from tfrecord.torch.dataset import TFRecordDataset
from torch.utils.data import DataLoader
from torch_geometric.data import Data
from torch_geometric.data.batch import Batch
import torch_geometric.transforms as T

from threading import Lock, Thread

import json
import h5py

loaded_meta = False
shapes = {}
dtypes = {}
types = {}
steps = None
dataset_dir = ""
batch_size = 1000
add_target = False

# datasets` dtype description
description = {
    "node_type": "byte",
    "mesh_pos": "byte",
    "cells_node": "byte",
    "cell_factor": "byte",
    "centroid": "byte",
    "face": "byte",
    "face_type": "byte",
    "face_length": "byte",
    "neighbour_cell": "byte",
    "cells_face": "byte",
    "cells_type": "byte",
    "cells_area": "byte",
    "unit_norm_v": "byte",
    "target|velocity_on_node": "byte",
    "target|pressure_on_node": "byte",
    "mean_u": "byte",
    "charac_scale": "byte",
}


# Iterative cfd dataset
class CFDdatasetIt(IterableDataset):
    def __init__(
        self,
        params,
        path,
        split,
        description=description,
        max_epochs=600,
        add_target=False,
    ):
        super().__init__()
        self.add_target = add_target
        self.steps = max_epochs
        self.path = path
        self.split = split
        self.dataset_dir = path
        self.params = params
        self.tfrecord_path = os.path.join(path, split + ".tfrecord")

        # index is generated by tfrecord2idx
        self.index_path = os.path.join(path, split + ".idx")
        self.file_handle = TFRecordDataset(
            self.tfrecord_path,
            self.index_path,
            description=description,
            transform=self.process_trajectory,
        )
        self.load_meta = False
        # self.graph_node_keys=['mesh_pos','face']
        # self.graph_edge_keys=['cells_face']
        # self.graph_cell_keys=['centroid','neighbour_cell','cells_area','cell_factor','target|velocity_on_node','target|pressure_on_node','cells_type','unit_norm_v','face_types']

    def __iter__(self):
        return iter(self.file_handle)

    def cal_relonyds_number(self, trajectory, params):
        """prepare data for cal_relonyds_number"""
        target_on_node = torch.cat(
            (
                trajectory["target|velocity_on_node"][0],
                trajectory["target|pressure_on_node"][0],
            ),
            dim=1,
        )
        edge_index = trajectory["face"][0]
        target_on_edge = (
            torch.index_select(target_on_node, 0, edge_index[0])
            + torch.index_select(target_on_node, 0, edge_index[1])
        ) / 2.0
        face_type = trajectory["face_type"][0].view(-1)
        node_type = trajectory["node_type"][0].view(-1)
        Inlet = target_on_edge[face_type == NodeType.INFLOW][:, 0]
        face_length = trajectory["face_length"][0][:, 0][face_type == NodeType.INFLOW]
        total_u = torch.sum(Inlet * face_length)
        mesh_pos = trajectory["mesh_pos"][0]
        top = torch.max(mesh_pos[:, 1]).numpy()
        bottom = torch.min(mesh_pos[:, 1]).numpy()
        left = torch.min(mesh_pos[:, 0]).numpy()
        right = torch.max(mesh_pos[:, 0]).numpy()
        mean_u = total_u / (top - bottom)

        """cal cylinder diameter"""
        boundary_pos = mesh_pos[node_type == NodeType.WALL_BOUNDARY].numpy()
        cylinder_mask = torch.full((boundary_pos.shape[0], 1), True).view(-1).numpy()
        cylinder_not_mask = np.logical_not(cylinder_mask)
        cylinder_mask = np.where(
            (
                (boundary_pos[:, 1] > bottom)
                & (boundary_pos[:, 1] < top)
                & (boundary_pos[:, 0] < right)
                & (boundary_pos[:, 0] > left)
            ),
            cylinder_mask,
            cylinder_not_mask,
        )
        # cylinder_pos = torch.from_numpy(boundary_pos[cylinder_mask])
        boundary_pos_obstacle = torch.from_numpy(boundary_pos[cylinder_mask])
        # _,_,R,_= hyper_fit(np.asarray(cylinder_pos))
        # L0 = R*2.
        _, wingleft_index = torch.max(boundary_pos_obstacle[:, 0:1], dim=0)
        _, winglright_index = torch.min(boundary_pos_obstacle[:, 0:1], dim=0)

        L0 = torch.norm(
            boundary_pos_obstacle[wingleft_index]
            - boundary_pos_obstacle[winglright_index]
        )

        rho = params.rho
        mu = params.mu
        trajectory["reynolds_num"] = (mean_u * L0 * rho) / mu
        return trajectory

    def process_trajectory(self, trajectory_data):
        self.shapes = {}
        self.dtypes = {}
        self.types = {}
        self.steps = 600
        if not loaded_meta:
            try:
                with open(os.path.join(self.dataset_dir, "meta.json"), "r") as fp:
                    meta = json.loads(fp.read())
                self.shapes = {}
                self.dtypes = {}
                self.types = {}
                self.steps = meta["trajectory_length"] - 2
                for key, field in meta["features"].items():
                    self.shapes[key] = field["shape"]
                    self.dtypes[key] = field["dtype"]
                    self.types[key] = field["type"]
            except FileNotFoundError as e:
                print(e)
                quit()
            self.load_meta = True
        trajectory = {}
        # decode bytes into corresponding dtypes
        for key, field in meta["features"].items():
            raw_data = trajectory_data[key].tobytes()
            mature_data = np.frombuffer(raw_data, dtype=getattr(np, field["dtype"]))
            mature_data = torch.from_numpy(mature_data.copy())
            reshaped_data = torch.reshape(mature_data, field["shape"])
            if key == "face":
                pass
            else:
                if field["type"] == "static":
                    reshaped_data = torch.tile(reshaped_data, (1, 1, 1))
                elif field["type"] == "dynamic_varlen":
                    pass
                elif field["type"] == "dynamic":
                    pass
                elif field["type"] != "dynamic":
                    raise ValueError("invalid data format")
            trajectory[key] = reshaped_data

            if not trajectory:
                print("trajectory is empty")

        trajectory = self.cal_relonyds_number(trajectory, self.params)

        return trajectory


class CFDdataset(TFRecordDataset):
    def __init__(
        self,
        params,
        path,
        split,
        description=description,
        max_epochs=600,
        add_target=False,
    ):
        super().__init__(
            os.path.join(path, split + ".tfrecord"),
            os.path.join(path, split + ".idx"),
            description,
            shuffle_queue_size=40,
            transform=self.process_trajectory,
        )
        self.add_target = add_target
        self.steps = max_epochs
        self.path = path
        self.split = split
        self.dataset_dir = path
        self.params = params
        self._epoch = 0
        self.tfrecord_path = os.path.join(path, split + ".tfrecord")
        # index is generated by tfrecord2idx
        self.index_path = os.path.join(path, split + ".idx")

    def collate(self, batch_list):
        return batch_list

    def set_epoch(self, epoch):
        self._epoch = epoch

    def process_trajectory(self, trajectory_data):
        self.shapes = {}
        self.dtypes = {}
        self.types = {}
        self.steps = 600
        if not loaded_meta:
            try:
                with open(os.path.join(self.dataset_dir, "meta.json"), "r") as fp:
                    meta = json.loads(fp.read())
                self.shapes = {}
                self.dtypes = {}
                self.types = {}
                self.steps = meta["trajectory_length"] - 2
                for key, field in meta["features"].items():
                    self.shapes[key] = field["shape"]
                    self.dtypes[key] = field["dtype"]
                    self.types[key] = field["type"]
            except FileNotFoundError as e:
                print(e)
                quit()
            self.load_meta = True
        trajectory = {}
        # decode bytes into corresponding dtypes
        for key, field in meta["features"].items():
            raw_data = trajectory_data[key].tobytes()
            mature_data = np.frombuffer(raw_data, dtype=getattr(np, field["dtype"]))
            mature_data = torch.from_numpy(mature_data.copy())
            reshaped_data = torch.reshape(mature_data, field["shape"])
            if key == "face":
                pass
            else:
                if field["type"] == "static":
                    reshaped_data = torch.tile(reshaped_data, (1, 1, 1))
                elif field["type"] == "dynamic_varlen":
                    pass
                elif field["type"] != "dynamic":
                    raise ValueError("invalid data format")
            trajectory[key] = reshaped_data

            if not trajectory:
                print("trajectory is empty")

        def _build_graph(graj):

            minibatch_data = graj
            """Optional node attr"""
            # node_attr = minibatch_data['mesh_pos'][0]
            face_node = minibatch_data["face"][0].to(torch.long)
            cells_node = torch.as_tensor(minibatch_data["cells_node"][0]).to(torch.long)

            # cell_attr
            edge_neighbour_cell = torch.as_tensor(
                minibatch_data["neighbour_cell"][0], dtype=torch.long
            )
            cell_area = torch.as_tensor(
                minibatch_data["cells_area"][0], dtype=torch.float32
            )
            centroid = torch.as_tensor(
                minibatch_data["centroid"][0], dtype=torch.float32
            )
            target_on_node = torch.cat(
                (
                    minibatch_data["target|velocity_on_node"],
                    minibatch_data["target|pressure_on_node"],
                ),
                dim=2,
            )
            cell_factor = torch.as_tensor(
                minibatch_data["cell_factor"][0], dtype=torch.float32
            )

            cells_type = minibatch_data["cells_type"][0].to(torch.long)
            cell_attr = cells_type  # lets use every epoch ground truth velocity as the cell attr
            cell_face = torch.as_tensor(
                minibatch_data["cells_face"][0], dtype=torch.long
            )

            # edge_attr
            face_length = minibatch_data["face_length"][0].to(torch.float32)
            face_types = minibatch_data["face_type"][0].to(torch.long)
            unv = torch.as_tensor(
                minibatch_data["unit_norm_v"][0], dtype=torch.float32
            )  # unit norm vector

            graph_node = Data(
                x=torch.cat(
                    (target_on_node[self.epoch], target_on_node[self.epoch + 1]), dim=1
                ),
                edge_index=face_node,
                face=cells_node.T,
            )
            graph_edge = Data(
                x=torch.cat((face_types, face_length), dim=1),
                face=cell_face.T,
                y=face_types,
            )
            graph_cell = Data(
                x=cell_attr,
                edge_index=edge_neighbour_cell,
                unv=unv,
                centroid=centroid,
                cell_area=cell_area,
                cell_factor=cell_factor,
                pos=centroid,
                y=cell_attr,
            )

            return graph_node, graph_edge, graph_cell

        return _build_graph(trajectory)


class Data_Pool(Dataset):

    def __init__(
        self,
        params=None,
        is_traning=True,
        device=None,
        transform=None,
        pre_transform=None,
    ):
        super().__init__(None, transform, pre_transform)
        self.params = params
        self.batch_size = params.batch_size
        if is_traning:
            self.dataset_size = params.dataset_size
            self.graph_indices = np.linspace(0, 999, 1000, endpoint=True).astype(
                np.int32
            )
            np.random.shuffle(self.graph_indices)
        else:
            self.dataset_size = 1
            self.graph_indices = np.linspace(0, 99, 100, endpoint=True).astype(np.int32)
        self.is_training = False
        self.max_epochs = params.n_epochs
        self.lock = Lock()
        self.pool = []
        self.mbatch_graph_node = []
        self.mbatch_graph_cell = []
        self.mbatch_graph_edge = []
        self.target = 0
        self.device = device
        self._device = device
        self.dataset_size = params.dataset_size
        self.train_epoch = 0
        self._time_step_list = (
            torch.from_numpy(
                np.random.permutation(self.params.train_traj_length).astype(np.int64)
            )
            .view(1, -1)
            .repeat(self.dataset_size, 1)
            .numpy()
        )

        # do not change this value
        self.epoch = 0

    def _set_epoch(self, epoch):
        self.epoch = epoch

    def _set_fetch_time_steps(self, epoch):
        self.train_epoch = epoch

    def _set_status(self, is_training=False):
        self.is_training = is_training

    def _set_time_step_list(self, new_time_step_list: np.ndarray):
        self._time_step_list = new_time_step_list

    def _set_dataset(self, state="simple"):
        """
        state= simple/complex/full
        """
        if state == "full":
            if (len(self.pool) == self.params.dataset_size) or not self.is_training:
                return True
            else:
                self.pool = self.complex_pool + self.simple_pool

        if state == "simple":
            if len(self.simple_pool) == 0:
                for index in self.simple_pool_index:
                    self.simple_pool.append(self.pool[index])

            if len(self.complex_pool) == 0:
                for index in self.complex_pool_index:
                    self.complex_pool.append(self.pool[index])

            self.pool = self.simple_pool

        elif state == "complex":
            if len(self.simple_pool) == 0:
                for index in self.simple_pool_index:
                    self.simple_pool.append(self.pool[index])

            if len(self.complex_pool) == 0:
                for index in self.complex_pool_index:
                    self.complex_pool.append(self.pool[index])

            self.pool = self.complex_pool

    def randbool(self, *size):
        """Returns 50% channce of True of False"""
        return torch.randint(2, size, device=self.device) == torch.randint(
            2, size, device=self.device
        )

    def _debug_time_steps(self):
        np.savetxt(
            "/home/litianyu/mycode/debug/time_steps_logger_"
            + str(self.train_epoch)
            + ".txt",
            self._time_step_list,
            fmt="%d",
            delimiter=",",
        )

    def random_time_steps(self, data_idx):
        choosed_time_steps = self._time_step_list[data_idx][
            self.train_epoch % self.params.train_traj_length
        ]
        return choosed_time_steps

    def len(self):
        return len(self.pool) * self.params.train_traj_length

    def get(self, idx):

        case_id = int(idx / self.params.train_traj_length)
        choosed_time_steps = idx % self.params.train_traj_length
        if self.params.dataset_type == "tf":
            minibatch_data = self.pool[case_id]
            cells_type = torch.as_tensor(minibatch_data["cells_type"][0]).to(torch.long)
            try:
                reynolds_num = torch.as_tensor(
                    minibatch_data["reynolds_num"][0]
                ).repeat(cells_type.shape[0], 1)
                
            except:
                mean_u = torch.as_tensor(minibatch_data["mean_u"][()]).view(-1,1).repeat(cells_type.shape[0], 1)
                
                R = torch.as_tensor(minibatch_data["charac_scale"][0]).repeat(
                    cells_type.shape[0], 1
                )
                reynolds_num = mean_u * 1.0 * R / 0.001
        elif self.params.dataset_type == "h5":
            minibatch_data = self.pool[str(case_id)]
            cells_type = torch.as_tensor(minibatch_data["cells_type"][0]).to(torch.long)
            try:
                reynolds_num = torch.as_tensor(
                    minibatch_data["reynolds_num"][()]
                ).view(-1,1).repeat(cells_type.shape[0], 1)
                
            except:
                mean_u = torch.as_tensor(minibatch_data["mean_u"][()]).view(-1,1).repeat(cells_type.shape[0], 1)
                
                R = torch.as_tensor(minibatch_data["charac_scale"][0]).repeat(
                    cells_type.shape[0], 1
                )
                reynolds_num = (mean_u * 1.0 * R / 0.001).to(torch.float32)
        else:
            raise ValueError("wrong dataset type")

        """Optional node attr"""
        mesh_pos = torch.as_tensor(minibatch_data["mesh_pos"][0])
        face_node = torch.as_tensor(minibatch_data["face"][0]).to(torch.long)
        cells_node = torch.as_tensor(minibatch_data["cells_node"][0]).to(torch.long)
        node_type = torch.as_tensor(minibatch_data["node_type"][0])
        target_on_node = torch.cat(
            torch.unbind(
                torch.cat(
                    (
                        torch.as_tensor(
                            minibatch_data["target|velocity_on_node"][
                                choosed_time_steps : choosed_time_steps + 2
                            ]
                        ),
                        torch.as_tensor(
                            minibatch_data["target|pressure_on_node"][
                                choosed_time_steps : choosed_time_steps + 2
                            ]
                        ),
                    ),
                    dim=2,
                )
            ),
            dim=1,
        )

        # cell_attr
        edge_neighbour_cell = torch.as_tensor(
            minibatch_data["neighbour_cell"][0], dtype=torch.long
        )
        cell_area = torch.as_tensor(
            minibatch_data["cells_area"][0], dtype=torch.float32
        )
        centroid = torch.as_tensor(minibatch_data["centroid"][0], dtype=torch.float32)

        cell_factor = torch.as_tensor(
            minibatch_data["cell_factor"][0], dtype=torch.float32
        )
        cell_face = torch.as_tensor(minibatch_data["cells_face"][0], dtype=torch.long)
        cells_type = torch.as_tensor(minibatch_data["cells_type"][0]).to(torch.long)
        # cell_attr = torch.cat((cells_type,reynolds_num),dim=1)
        cell_attr = torch.cat((cells_type, torch.zeros_like(cells_type)), dim=1)

        # edge_attr
        face_length = torch.as_tensor(minibatch_data["face_length"][0]).to(
            torch.float32
        )
        face_types = torch.as_tensor(minibatch_data["face_type"][0]).to(torch.long)
        unv = torch.as_tensor(
            minibatch_data["unit_norm_v"][0], dtype=torch.float32
        )  # unit norm vector

        if not self.is_training:
            graph_node = Data(
                x=target_on_node,
                edge_index=face_node,
                face=cells_node.T,
                pos=mesh_pos,
                node_type=node_type,
            )
        else:
            graph_node = Data(
                x=target_on_node, edge_index=face_node, face=cells_node.T, pos=mesh_pos
            )

        graph_edge = Data(
            x=torch.cat((face_types, face_length), dim=1),
            face=cell_face.T,
            y=face_types,
        )

        graph_cell = Data(
            x=cell_attr,
            edge_index=edge_neighbour_cell,
            unv=unv,
            centroid=centroid,
            cell_area=cell_area,
            cell_factor=cell_factor,
            pos=centroid,
            y=reynolds_num,
            graph_index=torch.as_tensor(idx),
        )

        return [graph_node, graph_edge, graph_cell]

    def _build_graph(self, minibatch_data, graph_indices):

        if self.params.dataset_type == "tf":
            minibatch_data = self.pool[graph_indices]
            cells_type = torch.as_tensor(minibatch_data["cells_type"][0]).to(torch.long)
            try:
                reynolds_num = torch.as_tensor(
                    minibatch_data["reynolds_num"][0]
                ).repeat(cells_type.shape[0], 1)
            except:
                mean_u = torch.as_tensor(minibatch_data["mean_u"][()]).view(-1,1).repeat(cells_type.shape[0], 1)
                R = torch.as_tensor(minibatch_data["charac_scale"][0]).repeat(
                    cells_type.shape[0], 1
                )
                reynolds_num = mean_u * 1.0 * R / 0.001
        elif self.params.dataset_type == "h5":
            minibatch_data = self.pool[str(graph_indices)]
            cells_type = torch.as_tensor(minibatch_data["cells_type"][0]).to(torch.long)
            try:
                reynolds_num = torch.as_tensor(
                    minibatch_data["reynolds_num"][()]
                ).view(-1,1).repeat(cells_type.shape[0], 1)
            except:
                mean_u = torch.as_tensor(minibatch_data["mean_u"][()]).view(-1,1).repeat(cells_type.shape[0], 1)
                R = torch.as_tensor(minibatch_data["charac_scale"][0]).repeat(
                    cells_type.shape[0], 1
                )
                reynolds_num = (mean_u * 1.0 * R / 0.001).to(torch.float32)
        else:
            raise ValueError("wrong dataset type")
        """Optional node attr"""
        mesh_pos = torch.as_tensor(minibatch_data["mesh_pos"][0])
        face_node = torch.as_tensor(minibatch_data["face"][0]).to(torch.long)
        cells_node = torch.as_tensor(minibatch_data["cells_node"][0]).to(torch.long)
        node_type = torch.as_tensor(minibatch_data["node_type"][0])
        target_on_node = torch.cat(
            (
                torch.as_tensor(minibatch_data["target|velocity_on_node"][:]),
                torch.as_tensor(minibatch_data["target|pressure_on_node"][:]),
            ),
            dim=2,
        ).transpose(0, 1)

        # cell_attr
        edge_neighbour_cell = torch.as_tensor(
            minibatch_data["neighbour_cell"][0], dtype=torch.long
        )
        cell_area = torch.as_tensor(
            minibatch_data["cells_area"][0], dtype=torch.float32
        )
        centroid = torch.as_tensor(minibatch_data["centroid"][0], dtype=torch.float32)

        cell_factor = torch.as_tensor(
            minibatch_data["cell_factor"][0], dtype=torch.float32
        )
        cell_face = torch.as_tensor(minibatch_data["cells_face"][0], dtype=torch.long)
        # cell_attr = torch.cat((cells_type,reynolds_num),dim=1)
        cell_attr = torch.cat((cells_type, torch.zeros_like(cells_type)), dim=1)
        mean_u = torch.as_tensor(minibatch_data["mean_u"][()]).view(-1,1).repeat(cells_type.shape[0], 1)

        # edge_attr
        face_length = torch.as_tensor(minibatch_data["face_length"][0]).to(
            torch.float32
        )
        face_types = torch.as_tensor(minibatch_data["face_type"][0]).to(torch.long)
        unv = torch.as_tensor(
            minibatch_data["unit_norm_v"][0], dtype=torch.float32
        )  # unit norm vector

        graph_node = Data(
            x=target_on_node,
            edge_index=face_node,
            face=cells_node.T,
            pos=mesh_pos,
            node_type=node_type,
        )

        graph_edge = Data(
            x=torch.cat((face_types, face_length), dim=1),
            face=cell_face.T,
            y=face_types,
        )

        graph_cell = Data(
            x=cell_attr,
            edge_index=edge_neighbour_cell,
            unv=unv,
            centroid=centroid,
            cell_area=cell_area,
            cell_factor=cell_factor,
            pos=centroid,
            y=cell_attr,
            graph_index=torch.as_tensor(graph_indices),
            mean_u=mean_u,
        )

        # keeping cell_face is import,because we need cell_face_index to calculate cell area
        # self.lock.acquire()
        self.mbatch_graph_node.append(graph_node)
        self.mbatch_graph_cell.append(graph_cell)
        self.mbatch_graph_edge.append(graph_edge)
        # self.lock.release()

    def reset(self):
        self.mbatch_graph_node.clear()
        self.mbatch_graph_cell.clear()
        self.mbatch_graph_edge.clear()

    def collate(self, batch_list):
        return batch_list

    def load_mesh_to_cpu(self, mode="tf", split="train", dataset_dir=None):
        if mode == "tf":
            self.simple_pool = []
            self.complex_pool = []
            self.simple_pool_index = []
            self.complex_pool_index = []
            if split == "train":
                test_ds = CFDdatasetIt(self.params, self.params.dataset_dir, "train")
            elif split == "valid":
                test_ds = CFDdatasetIt(self.params, self.params.dataset_dir, "valid")
            elif split == "test":
                test_ds = CFDdatasetIt(self.params, self.params.dataset_dir, "test")
            else:
                test_ds = CFDdatasetIt(self.params, self.params.dataset_dir, split)
            ds_loader = DataLoader(
                test_ds,
                batch_size=25,
                num_workers=4,
                prefetch_factor=20,
                pin_memory=False,
                collate_fn=lambda x: x,
            )
            print("loading whole dataset to cpu")
            # self.pool = list(test_ds)
            for traj_index, trajs in enumerate(ds_loader):
                tmp = list(trajs)
                if traj_index == 0:
                    self.pool = tmp
                else:
                    self.pool += tmp
                index = 0
                for traj in tmp:
                    if traj["reynolds_num"] < 100:
                        self.simple_pool_index.append(
                            int(len(self.pool) - len(tmp)) + index
                        )
                    else:
                        self.complex_pool_index.append(
                            int(len(self.pool) - len(tmp)) + index
                        )
                    index += 1
            return self.params.dataset_dir

        elif mode == "h5":
            self.valid_pool = []
            if dataset_dir is not None:
                self.pool = h5py.File(dataset_dir + f"/{split}.h5", "r")
            else:
                self.pool = h5py.File(self.params.dataset_dir_h5 + f"/{split}.h5", "r")

            return self.params.dataset_dir_h5
        """
        TODO: reset all graph to the same size                        UPDATE: not necessary
        min_node = 3000
        max_node = 0
        for i in range(1000):
            min_node = min(self.pool[i]['mesh_pos'].shape[1],min_node)
            max_node = max(self.pool[i]['mesh_pos'].shape[1],max_node)
            stastic(self.pool[i]['node_type'])
        print("min_node {0}, max_node {1}".format(min_node,max_node))
        """

    def require_minibatch_mesh(
        self,
        start_epoch=None,
        batch_index: list = None,
        is_training=False,
        dual_edge=False,
        mode="tf",
    ):
        self.is_training = is_training
        self.start_epoch = start_epoch % 599
        minibatch_threadpool = []
        for i in batch_index:
            if self.params.dataset_type == "tf":
                p = Thread(
                    target=self._build_graph,
                    args=(self.pool[i if mode == "tf" else str(i)], i),
                )
            elif self.params.dataset_type == "h5":
                p = Thread(target=self._build_graph, args=(self.pool[str(i)], i))
            else:
                raise ValueError("wrong dataset type")
            minibatch_threadpool.append(p)
            p.start()
            p.join()

        return self.valid_datapreprocessing(
            Batch.from_data_list(self.mbatch_graph_node).cuda(self.device),
            Batch.from_data_list(self.mbatch_graph_edge).cuda(self.device),
            Batch.from_data_list(self.mbatch_graph_cell).cuda(self.device),
            cell_noise=None,
            dual_edge=dual_edge,
        )

    def require_one_mesh(
        self, start_epoch, rollout_index, is_training=False, dual_edge=False
    ):
        self.is_training = is_training
        self.start_epoch = start_epoch % 599

        if self.params.dataset_type == "tf":
            self._build_graph(self.pool[rollout_index], rollout_index)
        elif self.params.dataset_type == "h5":
            self._build_graph(self.pool[str(rollout_index)], rollout_index)
        else:
            raise ValueError("wrong dataset type")

        return self.valid_datapreprocessing(
            Batch.from_data_list(self.mbatch_graph_node).cuda(self.device),
            Batch.from_data_list(self.mbatch_graph_edge).cuda(self.device),
            Batch.from_data_list(self.mbatch_graph_cell).cuda(self.device),
            cell_noise=None,
            dual_edge=dual_edge,
        )

    def enforce_boundary_condition(
        self,
        predicted_cell_attr=None,
        bc=None,
        edge_neighbour_index=None,
        cells_type=None,
        face_type=None,
        dim=0,
    ):

        # INFLOW
        mask_face_inflow = face_type == NodeType.INFLOW

        edge_neighbour_index_l = edge_neighbour_index[0]
        edge_neighbour_index_r = edge_neighbour_index[1]

        mask_inflow_ghost_cell = (
            cells_type[edge_neighbour_index_l[mask_face_inflow]]
            == NodeType.GHOST_INFLOW
        )
        # dim=0 stands for interior cell, dim=1 stands for ghost cell
        edge_neighbour_index_inflow = edge_neighbour_index[:, mask_face_inflow].clone()
        edge_neighbour_index_inflow[0, mask_inflow_ghost_cell] = edge_neighbour_index_r[
            mask_face_inflow
        ][mask_inflow_ghost_cell]
        edge_neighbour_index_inflow[1, mask_inflow_ghost_cell] = edge_neighbour_index_l[
            mask_face_inflow
        ][mask_inflow_ghost_cell]

        # constant padding at inflow boundary
        if dim == 1:
            predicted_cell_attr[:, edge_neighbour_index_inflow[1], :] = bc[
                :, mask_face_inflow, 0:2
            ]
        else:
            predicted_cell_attr[edge_neighbour_index_inflow[1], :] = bc[
                mask_face_inflow, 0:2
            ]

        # WALL BOUNDARY
        mask_face_wall = face_type == NodeType.WALL_BOUNDARY
        mask_wall_ghost_cell = (
            cells_type[edge_neighbour_index_l[mask_face_wall]] == NodeType.GHOST_WALL
        )
        # dim=0 stands for interior cell, dim=1 stands for ghost cell
        edge_neighbour_index_wall = edge_neighbour_index[:, mask_face_wall].clone()
        edge_neighbour_index_wall[0, mask_wall_ghost_cell] = edge_neighbour_index_r[
            mask_face_wall
        ][mask_wall_ghost_cell]
        edge_neighbour_index_wall[1, mask_wall_ghost_cell] = edge_neighbour_index_l[
            mask_face_wall
        ][mask_wall_ghost_cell]

        # inverse interior flux to ghost cell at wall boundary
        if dim == 1:
            predicted_cell_attr[:, edge_neighbour_index_wall[1], :] = 0.0
        else:
            predicted_cell_attr[edge_neighbour_index_wall[1], :] = 0.0

        # OUTFLOW
        mask_face_outflow = face_type == NodeType.OUTFLOW
        mask_outflow_ghost_cell = (
            cells_type[edge_neighbour_index_l[mask_face_outflow]]
            == NodeType.GHOST_OUTFLOW
        )
        # dim=0 stands for interior cell, dim=1 stands for ghost cell
        edge_neighbour_index_outflow = edge_neighbour_index[
            :, mask_face_outflow
        ].clone()
        edge_neighbour_index_outflow[0, mask_outflow_ghost_cell] = (
            edge_neighbour_index_r[mask_face_outflow][mask_outflow_ghost_cell]
        )
        edge_neighbour_index_outflow[1, mask_outflow_ghost_cell] = (
            edge_neighbour_index_l[mask_face_outflow][mask_outflow_ghost_cell]
        )

        # interior equal ghost cell at outflow
        if dim == 1:
            predicted_cell_attr[:, edge_neighbour_index_outflow[1], :] = (
                predicted_cell_attr[:, edge_neighbour_index_outflow[0], :].clone()
            )
        else:
            predicted_cell_attr[edge_neighbour_index_outflow[1], :] = (
                predicted_cell_attr[edge_neighbour_index_outflow[0], :].clone()
            )

        return predicted_cell_attr

    def valid_datapreprocessing(
        self, graph_node, graph_edge, graph_cell, cell_noise, dual_edge=False
    ):

        target_on_cell = (
            (graph_cell.cell_factor[:, 0:1]).unsqueeze(1)
            * (torch.index_select(graph_node.x, 0, graph_node.face[0]))
            + (graph_cell.cell_factor[:, 1:2]).unsqueeze(1)
            * (torch.index_select(graph_node.x, 0, graph_node.face[1]))
            + (graph_cell.cell_factor[:, 2:3]).unsqueeze(1)
            * (torch.index_select(graph_node.x, 0, graph_node.face[2]))
        )

        target_on_edge = (
            torch.index_select(graph_node.x, 0, graph_node.edge_index[0])
            + torch.index_select(graph_node.x, 0, graph_node.edge_index[1])
        ) / 2.0

        # There`s No target pressure on cell during training, but in testing
        graph_cell.y = target_on_cell[:, self.start_epoch + 1 :, :]
        graph_edge.y = target_on_edge[:, self.start_epoch + 1 :, :]

        # permute the direction of edges to model robust
        senders = graph_cell.edge_index[0]
        receivers = graph_cell.edge_index[1]

        mask_face_interior = torch.logical_or(
            graph_edge.x[:, 0] == utilities.NodeType.NORMAL,
            graph_edge.x[:, 0] == utilities.NodeType.OUTFLOW,
        )
        mask_face_boundary = torch.logical_not(mask_face_interior)

        if dual_edge:
            twoway_senders = torch.cat((senders, receivers), dim=0)
            twoway_recivers = torch.cat((receivers, senders), dim=0)
            None_noised_node_uv = target_on_cell[:, self.start_epoch, 0:2]
            graph_cell.x = torch.cat(
                (graph_cell.x, None_noised_node_uv), dim=1
            )  # we don`t need pressure as input
            relative_mesh_pos = (
                graph_cell.pos[twoway_senders] - graph_cell.pos[twoway_recivers]
            )
            init_edge_uv = (
                graph_cell.x[twoway_senders, 2:4] - graph_cell.x[twoway_recivers, 2:4]
            )
            init_edge_uv[mask_face_boundary, 0:2] = target_on_edge[
                mask_face_boundary, 0, 0:2
            ]
            graph_cell.edge_attr = torch.cat(
                (init_edge_uv, relative_mesh_pos, graph_edge.x[:, 1:2].repeat(2, 1)),
                dim=1,
            )
        else:
            None_noised_node_uv = target_on_cell[:, self.start_epoch, 0:2]
            graph_cell.x = torch.cat(
                (graph_cell.x, None_noised_node_uv), dim=1
            )  # we don`t need pressure as input
            relative_mesh_pos = torch.index_select(
                graph_cell.pos, 0, senders
            ) - torch.index_select(graph_cell.pos, 0, receivers)
            init_edge_uv = torch.index_select(
                graph_cell.x[:, 2:4], 0, senders
            ) - torch.index_select(graph_cell.x[:, 2:4], 0, receivers)
            # enforce BC at boundary cell`s face
            init_edge_uv[mask_face_boundary, 0:2] = target_on_edge[
                mask_face_boundary, 0, 0:2
            ]
            graph_cell.edge_attr = torch.cat(
                (init_edge_uv, relative_mesh_pos, graph_edge.x[:, 1:2]), dim=1
            )

        return [
            graph_node,
            graph_edge,
            graph_cell,
            mask_face_interior,
            mask_face_boundary,
        ]

    def train_datapreprocessing(self, graph_list, dual_edge=False):

        graph_list_cuda = self.cudacopy(graph_list)

        graph_node, graph_edge, graph_cell, cell_noise = self.get_noise(graph_list_cuda)
        # datapreprocessing
        target_on_cell = (
            graph_cell.cell_factor[:, 0:1]
            * (torch.index_select(graph_node.x, 0, graph_node.face[0]))
            + graph_cell.cell_factor[:, 1:2]
            * (torch.index_select(graph_node.x, 0, graph_node.face[1]))
            + graph_cell.cell_factor[:, 2:3]
            * (torch.index_select(graph_node.x, 0, graph_node.face[2]))
        )

        target_on_edge = (
            torch.index_select(graph_node.x, 0, graph_node.edge_index[0])
            + torch.index_select(graph_node.x, 0, graph_node.edge_index[1])
        ) / 2.0

        # There`s No target pressure on cell during training, but in testing
        if self.is_training:
            graph_cell.y = target_on_cell[:, 3:5]
        else:
            graph_cell.y = target_on_cell[:, 3:6]
        graph_edge.y = target_on_edge[:, 3:6]

        # permute the direction of edges to model robust
        senders = graph_cell.edge_index[0]
        receivers = graph_cell.edge_index[1]

        mask_face_interior = torch.logical_or(
            graph_edge.x[:, 0] == utilities.NodeType.NORMAL,
            graph_edge.x[:, 0] == utilities.NodeType.OUTFLOW,
        )
        mask_face_boundary = torch.logical_not(mask_face_interior)

        if dual_edge:
            twoway_senders = torch.cat((senders, receivers), dim=0)
            twoway_recivers = torch.cat((receivers, senders), dim=0)
            noised_cell_attr = target_on_cell[:, 0:2] + cell_noise
            graph_cell.x = torch.cat(
                (graph_cell.x, noised_cell_attr), dim=1
            )  # we don`t need pressure as input
            relative_mesh_pos = (
                graph_cell.pos[twoway_senders] - graph_cell.pos[twoway_recivers]
            )
            init_edge_uv = (
                graph_cell.x[twoway_senders, 2:4] - graph_cell.x[twoway_recivers, 2:4]
            )
            init_edge_uv[mask_face_boundary, 0:2] = target_on_edge[
                mask_face_boundary, 0:2
            ]
            graph_cell.edge_attr = torch.cat(
                (init_edge_uv, relative_mesh_pos, graph_edge.x[:, 1:2].repeat(2, 1)),
                dim=1,
            )

        else:
            noised_cell_attr = target_on_cell[:, 0:2] + cell_noise
            graph_cell.x = torch.cat(
                (graph_cell.x, noised_cell_attr), dim=1
            )  # we don`t need pressure as input
            random_mask = self.randbool(1, senders.shape[0]).repeat(2, 1)
            random_direction_edge = torch.where(
                random_mask,
                torch.stack((senders, receivers), dim=0),
                torch.stack((receivers, senders), dim=0),
            )
            relative_mesh_pos = torch.index_select(
                graph_cell.pos, 0, random_direction_edge[0]
            ) - torch.index_select(graph_cell.pos, 0, random_direction_edge[1])
            init_edge_uv = torch.index_select(
                graph_cell.x[:, 2:4], 0, random_direction_edge[0]
            ) - torch.index_select(graph_cell.x[:, 2:4], 0, random_direction_edge[1])
            
            # enforce BC at boundary cell`s face
            init_edge_uv[mask_face_boundary, 0:2] = target_on_edge[
                mask_face_boundary, 0:2
            ]
            graph_cell.edge_attr = torch.cat(
                (init_edge_uv, relative_mesh_pos, graph_edge.x[:, 1:2]), dim=1
            )

        # prepare for the next iteration
        return (
            graph_node,
            graph_edge,
            graph_cell,
            mask_face_interior,
            mask_face_boundary,
        )

    @staticmethod
    def create_next_graph(
        graph_old,
        graph_edge,
        mask_face_boundary,
        cell_next_UV,
        cell_type,
        Re,
        edge_RMP_EU,
        dual_edge=False,
    ):

        graph_old.x = torch.cat((cell_type, Re, cell_next_UV), dim=1)

        if dual_edge:
            twoway_senders = torch.cat(
                (graph_old.edge_index[0], graph_old.edge_index[1]), dim=0
            )
            twoway_recivers = torch.cat(
                (graph_old.edge_index[1], graph_old.edge_index[0]), dim=0
            )
            init_edge_uv = cell_next_UV[twoway_senders] - cell_next_UV[twoway_recivers]
            init_edge_uv[mask_face_boundary, 0:2] = graph_edge.y[
                mask_face_boundary, 0, 0:2
            ]
            graph_old.edge_attr = torch.cat((init_edge_uv, edge_RMP_EU), dim=1)

        else:
            init_edge_uv = (
                cell_next_UV[graph_old.edge_index[0]]
                - cell_next_UV[graph_old.edge_index[1]]
            )
            
            # enforce BC at boundary cell`s face
            init_edge_uv[mask_face_boundary, 0:2] = graph_edge.y[
                mask_face_boundary, 0, 0:2
            ]
            graph_old.edge_attr = torch.cat((init_edge_uv, edge_RMP_EU), dim=1)

        return graph_old

    def cudacopy(self, data):

        if type(data) is tuple:
            return [xi.cuda() for xi in data]

        if type(data) is list:
            return [xi.cuda() for xi in data]

        return data.cuda()

    def get_noise(self, graph_list):

        v_sequence_noise_on_cell = noise.get_v_noise_on_cell(
            graph_list[2],
            noise_std=self.params.Noise_injection_factor,
            outchannel=2,
            device=graph_list[2].x.device,
        )
        graph_list.append(v_sequence_noise_on_cell)
        return graph_list


def make_data_loader(
    data_pool=None,
    batch_size=None,
    num_workers=8,
    prefetch_factor=10,
    pin_memory=False,
    shuffle=False,
):
    from torch_geometric.loader import DataLoader

    def collate_fn(batch_list):
        assert type(batch_list) == list, f"Error"
        batch_size = len(batch_list)
        print("batch_size: %d" % batch_size)
        return list(batch_list)

    """
    Create a BRACS data loader
    """
    dataset = data_pool
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        pin_memory=pin_memory,
        collate_fn=collate_fn,
    )

    return dataloader


def extract_relonyds_number(graph_node, graph_edge):
    """prepare data for cal_relonyds_number"""
    target_on_node = graph_node.x[:, 0, 0:2].cpu()
    edge_index = graph_node.edge_index.cpu()
    target_on_edge = (
        torch.index_select(target_on_node, 0, edge_index[0])
        + torch.index_select(target_on_node, 0, edge_index[1])
    ) / 2.0
    face_type = graph_edge.x[:, 0:1].cpu().view(-1)
    node_type = graph_node.node_type.cpu().view(-1)
    Inlet = target_on_edge[face_type == utilities.NodeType.INFLOW][:, 0]
    face_length = graph_edge.x[:, 1:2].cpu()[:, 0][
        face_type == utilities.NodeType.INFLOW
    ]
    total_u = torch.sum(Inlet * face_length)
    mesh_pos = graph_node.pos.cpu()
    top = torch.max(mesh_pos[:, 1]).numpy()
    bottom = torch.min(mesh_pos[:, 1]).numpy()
    left = torch.min(mesh_pos[:, 0]).numpy()
    right = torch.max(mesh_pos[:, 0]).numpy()
    mean_u = total_u / (top - bottom)

    """cal cylinder diameter"""
    boundary_pos = mesh_pos[node_type == utilities.NodeType.WALL_BOUNDARY].numpy()
    cylinder_mask = torch.full((boundary_pos.shape[0], 1), True).view(-1).numpy()
    cylinder_not_mask = np.logical_not(cylinder_mask)
    cylinder_mask = np.where(
        (
            (boundary_pos[:, 1] > bottom)
            & (boundary_pos[:, 1] < top)
            & (boundary_pos[:, 0] < right)
            & (boundary_pos[:, 0] > left)
        ),
        cylinder_mask,
        cylinder_not_mask,
    )
    boundary_pos_obs = torch.from_numpy(boundary_pos[cylinder_mask])
    # _,_,R,_= hyper_fit(np.asarray(cylinder_pos))
    # L0 = R*2.
    _, wingleft_index = torch.max(boundary_pos_obs[:, 0:1], dim=0)
    _, winglright_index = torch.min(boundary_pos_obs[:, 0:1], dim=0)

    L0 = torch.norm(
        boundary_pos_obs[wingleft_index] - boundary_pos_obs[winglright_index]
    )

    rho = params.rho
    mu = params.mu
    Re = (mean_u * L0 * rho) / mu
    mean_u = mean_u
    return Re, mean_u


if __name__ == "__main__":
    # import sys
    # sys.path.append(os.path.join('/lvm_data/litianyu/mycode-new/GEP-FVGN/repos-py/FVM/my_FVNN/'))
    from utils.get_param import get_hyperparam
    from utils import get_param
    import time
    from torch_geometric.loader import DataLoader

    params = get_param.params()[0]
    params.dataset_type = "h5"
    split = "train"

    # initialize Training Dataset
    start = time.time()
    datasets = Data_Pool(params=params, is_traning=True, device="cuda")
    datasets._set_status(is_training=True)
    host_dataset_dir = datasets.load_mesh_to_cpu(
        mode="h5",
        split="valid",
        dataset_dir="/data/litianyu/dataset/MeshGN/cylinder_flow/mesh_h5",
    )

    # train_datasets._set_dataset("full")
    train_loader = DataLoader(
        datasets,
        batch_size=params.batch_size,
        num_workers=2,
        shuffle=True,
        persistent_workers=True,
    )
    # noise_std_list=np.array([2e-2,4e-2,6e-2])
    end = time.time()

    print("Training traj has been loaded time consuming:{0}".format(end - start))

    def loss_function(x):
        return torch.pow(x, 2)

    # Initialize variables for recording max and min Reynolds numbers
    max_re = -float("inf")
    min_re = float("inf")

    # training loop
    last_time = time.time()
    avg_time_1 = []
    re_num_list = []
    element_num_list = []
    mean_u_list = []

    plt.figure(figsize=(16, 9))
    for index in range(len(datasets.pool)):

        start = time.time()
        (
            mbatch_graph_node,
            mbatch_graph_edge,
            graph_old,
            mask_face_interior,
            mask_face_boundary,
        ) = datasets.require_minibatch_mesh(
            start_epoch=0, batch_index=[index], is_training=False, mode="h5"
        )
        element_num_list.append(graph_old.num_nodes)
        re_num, mean_u = extract_relonyds_number(mbatch_graph_node, mbatch_graph_edge)
        re_num_list.append(re_num)
        mean_u_list.append(mean_u)
        print("Relonyds number:", re_num.numpy())

        # Update max and min Reynolds numbers
        max_re = max(max_re, re_num)
        min_re = min(min_re, re_num)

        print("velocity max in x dir:", torch.max(mbatch_graph_node.x[:, 0]))
        const_time = time.time() - last_time
        avg_time_1.append(const_time)
        # print('time consuming:', const_time)
        last_time = time.time()
        datasets.mbatch_graph_node.clear()
        datasets.mbatch_graph_edge.clear()
        datasets.mbatch_graph_cell.clear()

    # Print the recorded max and min Reynolds numbers
    print("Maximum Reynolds number:", max_re.numpy())
    print("Minimum Reynolds number:", min_re.numpy())

    re_num_list = np.array(re_num_list)
    re_num_list = np.sort(re_num_list, axis=0)
    mean_u_list = np.array(mean_u_list)
    mean_u_list = np.sort(mean_u_list, axis=0)

    element_num_list = np.array(element_num_list)
    avg_element_num = np.average(element_num_list)
    min_element_num = np.min(element_num_list)
    max_element_num = np.max(element_num_list)
    print("avg_element_num:", avg_element_num)
    print("min_element_num:", min_element_num)
    print("max_element_num:", max_element_num)

    plt.hist(
        re_num_list, bins=20, density=False, alpha=0.6, color="blue", edgecolor="black"
    )
    # sns.kdeplot(re_num_list, color='red', linewidth=2)
    # plt.ylim(-1,1)
    plt.title("Relonyds number distribution")
    plt.xlabel("Relonyds number")
    plt.ylabel("Amounts")
    # 修改Y轴的刻度，使其只显示整数
    ax = plt.gca()
    ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))
    plt.savefig("Relonyds number distribution.png")
    plt.show()

    plt.figure(figsize=(16, 9))
    plt.hist(
        mean_u_list, bins=20, density=False, alpha=0.6, color="blue", edgecolor="black"
    )
    # sns.kdeplot(re_num_list, color='red', linewidth=2)
    # plt.ylim(-1,1)
    plt.title("Mean U distribution")
    plt.xlabel("Mean U")
    plt.ylabel("Amounts")
    plt.savefig("Mean U distribution.png")
    plt.show()

    print("MAX Relonyds number:", np.max(re_num_list))
    print("avg_time_2:", np.average(np.asarray(avg_time_1)))
    print("done loading dataset")
